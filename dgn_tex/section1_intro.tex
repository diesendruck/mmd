\section{Introduction}

GANs have become a popular method of simulating data from complex datasets, as an alternative to previous likelihood-based approaches. In classical and Bayesian modeling, generative models require a known probabilistic structure, where the functional form of the likelihood is known beforehand. The Gaussian mixture model is one case, where the number of Gaussian components and their parameters can be learned, but where data is always evaluated against a Gaussian likelihood. Such methods can be difficult to justify and intractable to compute in very high-dimensional space, where data points exist on a sparse sub-manifold of the data space. For the remainder of this paper, we refer to samples from the generative function/model as \textit{simulations}, and we refer to samples from the true data distribution as \textit{data}.

To address the challenges of likelihood intractability, a class of models called \textit{likelihood-free} models emerged, where GANs can be considered a new member. Approximate Bayesian Computation (ABC) is one such model, where simulations are generated from a function with randomly initialized parameters, then compared to data using a distance measure chosen by a practitioner. When the distance between simulations and data is below a threshold, that parameter set is stored, and the random parameter initialization begins again.

More recently, GANs have accomplished as similar kind of sampling, where a neural network classifier serves as a proxy distance function. By some interpretations, the classifier acts as a likelihood ratio estimator between simulations and data. Several results show smooth transitions between simulations in complex image spaces, illustrating realistic paths through sparse subspaces of the data.

Smooth interpolations of high-dimensional data may be useful for a variety of creative applications, such as design and animation, where a general latent structure is learned from data, and then explored using such a model. In the exploratory stage, new simulations may emerge, whose fundamental latent features match the data, but whose details have never been seen before. Consider the following two examples: 
\begin{itemize}
\item Learning the latent structure of a chair, using a large collection of images of chairs. Shared features include vertical legs, a flat base, and a vertical back. A generative model may train on these images, then simulate an image with a configuration of legs, base, and back that is entirely novel.\\
\item Learning to simulate faces for an avatar in virtual reality scene. Faces are learned from a large collection of human faces, and randomly simulated for each new session.
\end{itemize}
With the current modeling toolset, a generative function will simulate approximately the same distribution as the data. Consider, though, that a model user, designer, or practitioner may be interested in generating a distribution that (a) contains all modes, and (b) favors a particular class in a way that is different from the data. Supposing for the above two examples that the data under-represents rocking chairs and female faces. A user may want to generate all kinds of chairs, but twice as many rocking chairs as folding chairs. A game designer may want to boost the proportion of female faces by 30\%. The weighted MMD provides a way to incorporate these behaviors directly into the adversarial distance measure. 

The remainder of this paper is organized as follows: Section 2 introduces the GAN framework, mode collapse, and MMD-GAN. Section 3 describes our weighted MMD approach for bias correction. Section 4 demonstrates the use of the weighted MMD on one-dimensional toy data and MNIST data. Section 5 concludes with discussion and ideas for future work. 
