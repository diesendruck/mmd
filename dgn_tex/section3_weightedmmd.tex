\section{Weighted MMD}

The weighted MMD interprets the observed data distribution as a thinned version of the desired, unbiased distribution. For a desired, unbiased probability distribution $\mathbb{P}(x)$, we observe \textit{only} the modified distribution $T(x)\mathbb{P}(x)$, where $T(x): x \rightarrow t \in [0, 1]$ is a thinning function which discounts density for some region(s) of the input space $\mathcal{X}$. The original kernel mean embedding is then re-written as an expectation over a modified measure $T(x)\mathbb{P}(x)$, where:
\begin{align}
\mu_{T \cdot \mathbb{P}} = \int k(x,\cdot)dT(x)\mathbb{P}(x) = \int \frac{k(x,\cdot)}{T(x)}d\mathbb{P}(x).
\end{align}
A known under-representation of a particular region in the data can be expressed using a thinning function $T(x)$ centered around that region, indicating more of a discount ($t$ closer to zero) in that area, and less of a discount ($t$ close to one) elsewhere. Since our choice of $T(x)$ will not necessarily produce a valid probability measure $T(x)\mathbb{P}(x)$, we apply weights $1/T(x)$ as if from a self-normalizing importance sampler. The resulting MMD takes the following form:
\begin{align}
M_k(\mathbb{P}, \mathbb{Q}) &= ||\mu_{T \cdot \mathbb{P}} - \mu_{\mathbb{Q}}||^2_\mathcal{H} \\
&= \mathbb{E}_{T \cdot \mathbb{P}}\Big[\frac{1}{T(x)} k(x,x') \frac{1}{T(x')}\Big] - 2 \mathbb{E}_{T\cdot\mathbb{P}, \mathbb{Q}}\Big[\frac{1}{T(x)}k(x,y)\Big] + \mathbb{E}_{\mathbb{Q}}[k(y,y')].
\end{align}
\subsection{Defining the thinning function}
In order to center the thinning function around a target region, the practitioner selects a small sample of points $X^*$ which represent the region they would like to boost. A new kernel $k_{target}(\cdot)$ can be defined using the sample mean and sample covariance of the selected points, giving higher values for inputs near $X^*$, and lower values otherwise. By scaling kernel values $k_{target}(\cdot)$ to fall within $[0, k_{max}]: k_{max} \in [0,1)$, the thinning function can be defined as $T(x) = 1 - k_{target} : T(x) \in [1-k_{max}, 1]$. Taking $k_{max}=0.75$, for example, means that $k_{target}$ will reach its maximum of 0.75 near the target region. Similarly, thinning values will reach their lowest values of 0.25 near the target region. A thinning value of 0.25 near the target region implies that the desired, unbiased distribution $\mathbb{P}$ (as opposed to the observed $T(x)\mathbb{P}(x)$) is four times as dense in that region.

By taking kernel expectations with the weights described above, a practitioner can direct and control the density of the generative function, boosting target regions with minimal labeling, and scaling the effect to the desired level.
